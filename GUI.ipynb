{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtGui import QImage, QPixmap, QIcon\n",
    "from PyQt5.QtCore import QThread, pyqtSignal, Qt\n",
    "from PyQt5.QtWidgets import QApplication, QDialog, QLabel, QPushButton, QVBoxLayout, QFileDialog, QHBoxLayout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mediapipe_pose:\n",
    "    def __init__(self):\n",
    "        self.mp_holistic = mp.solutions.holistic \n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "    def mediapipe_detection(self,image,model):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "        image.flags.writeable = False                 \n",
    "        results = model.process(image)                 \n",
    "        image.flags.writeable = True                   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "        return image, results\n",
    "    def draw_styled_landmarks(self,image, results):\n",
    "        self.mp_drawing.draw_landmarks(image, results.pose_landmarks, self.mp_holistic.POSE_CONNECTIONS,\n",
    "                                 self.mp_drawing.DrawingSpec(color=(112,112,112), thickness=2, circle_radius=1), \n",
    "                                 self.mp_drawing.DrawingSpec(color=(94,200,0), thickness=2, circle_radius=1)\n",
    "                                 ) \n",
    "    def extract_keypoints(self,results):\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        return np.concatenate([pose])\n",
    "    def BBox(self,image,results):\n",
    "        xList,yList,bbox = [],[],[]\n",
    "        if results.pose_landmarks:\n",
    "            for id,land in enumerate(results.pose_landmarks.landmark):\n",
    "                h,w,c = image.shape # high,weight,chanel with img\n",
    "                cx = int(land.x *w)\n",
    "                cy = int(land.y *h)\n",
    "                xList.append(cx)\n",
    "                yList.append(cy)\n",
    "            xmin,xmax = min(xList),max(xList)\n",
    "            ymin,ymax = min(yList),max(yList)\n",
    "            bbox = xmin,ymin,xmax,ymax\n",
    "        return bbox\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oumai\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class VideoThread(QThread):\n",
    "    change_pixmap_signal = pyqtSignal(np.ndarray)\n",
    "    frame_processed_signal = pyqtSignal(np.ndarray)\n",
    "\n",
    "    def __init__(self, video_source=0):\n",
    "        super(VideoThread, self).__init__()\n",
    "        self.video_source = video_source\n",
    "        self.run_video = True\n",
    "\n",
    "    def run(self):\n",
    "        cap = cv2.VideoCapture(self.video_source)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "        while cap.isOpened() and self.run_video:\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                self.change_pixmap_signal.emit(frame)\n",
    "                self.frame_processed_signal.emit(frame)\n",
    "                time.sleep(0.033)\n",
    "            else:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "    def stop(self):\n",
    "        self.run_video = False\n",
    "\n",
    "class Ui_Dialog(QDialog):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Activity Recognition\")\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "        self.model = tf.keras.models.load_model('weight/LRCN_model.h5')\n",
    "\n",
    "        self.main_layout = QVBoxLayout()\n",
    "        self.video_label = QLabel(self)\n",
    "        self.video_label.setFixedSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"border: 1px solid black;\")\n",
    "        \n",
    "        self.control_layout = QHBoxLayout()\n",
    "        self.text_label = QLabel('Activity: ', self)\n",
    "        self.text_label.setStyleSheet(\"font-size: 16px; font-weight: bold;\")\n",
    "        self.fps_label = QLabel('FPS: ', self)\n",
    "        self.fps_label.setStyleSheet(\"font-size: 16px; font-weight: bold;\")\n",
    "\n",
    "        self.start_button = QPushButton('Start Camera', self)\n",
    "        self.start_button.setIcon(QIcon('image/start_icon.png'))\n",
    "        self.start_button.setStyleSheet(\"background-color: white; color: black; font-size: 14px;\")\n",
    "        self.start_button.clicked.connect(self.start_camera)\n",
    "\n",
    "        self.load_video_button = QPushButton('Load Video', self)\n",
    "        self.load_video_button.setIcon(QIcon('image/video_icon.png'))\n",
    "        self.load_video_button.setStyleSheet(\"background-color: white; color: black; font-size: 14px;\")\n",
    "        self.load_video_button.clicked.connect(self.load_video)\n",
    "\n",
    "        self.control_layout.addWidget(self.start_button)\n",
    "        self.control_layout.addWidget(self.load_video_button)\n",
    "        self.control_layout.addWidget(self.text_label)\n",
    "        self.control_layout.addWidget(self.fps_label)\n",
    "\n",
    "        self.main_layout.addWidget(self.video_label)\n",
    "        self.main_layout.addLayout(self.control_layout)\n",
    "        self.setLayout(self.main_layout)\n",
    "\n",
    "        self.sequence = []\n",
    "        self.actions = ['ApplyEyeMakeup','Basketball','Biking', 'BoxingPunchingBag','Eating', 'HorseRiding','Hugging','PlayingGuitar', 'Rafting','Salat','SalsaSpin','Waving', 'WritingOnBoard']\n",
    "        self.thread = None\n",
    "        self.pTime = time.time()\n",
    "\n",
    "    def start_camera(self):\n",
    "        if self.thread is not None:\n",
    "            self.thread.stop()\n",
    "        self.thread = VideoThread()\n",
    "        self.thread.change_pixmap_signal.connect(self.update_display)\n",
    "        self.thread.frame_processed_signal.connect(self.process_frame)\n",
    "        self.thread.start()\n",
    "\n",
    "    def load_video(self):\n",
    "        path, _ = QFileDialog.getOpenFileName(self, \"Open Video\", \"\", \"Video Files (*.mp4 *.flv *.ts *.mts *.avi)\")\n",
    "        if path:\n",
    "            if self.thread is not None:\n",
    "                self.thread.stop()\n",
    "            self.thread = VideoThread(video_source=path)\n",
    "            self.thread.change_pixmap_signal.connect(self.update_display)\n",
    "            self.thread.frame_processed_signal.connect(self.process_frame)\n",
    "            self.thread.start()\n",
    "\n",
    "    @QtCore.pyqtSlot(np.ndarray)\n",
    "    def update_display(self, frame):\n",
    "        \"\"\" Update video label to show new frame without unnecessary resizing. \"\"\"\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = frame_rgb.shape\n",
    "        bytes_per_line = ch * w\n",
    "        convert_to_Qt_format = QImage(frame_rgb.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "        pixmap = QPixmap.fromImage(convert_to_Qt_format)\n",
    "        self.video_label.setPixmap(pixmap.scaled(self.video_label.width(), self.video_label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "    @QtCore.pyqtSlot(np.ndarray)\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\" Process each frame for activity recognition. \"\"\"\n",
    "        frame_resized = cv2.resize(frame, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "        frame_normalized = frame_resized / 255.0\n",
    "        self.sequence.append(frame_normalized)\n",
    "        if len(self.sequence) == 20:\n",
    "            self.sequence = np.array(self.sequence)\n",
    "            self.sequence = self.sequence[np.newaxis, ...]\n",
    "            predictions = self.model.predict(self.sequence)\n",
    "            predicted_action = self.actions[np.argmax(predictions)]\n",
    "            self.text_label.setText('Activity: ' + predicted_action)\n",
    "            self.sequence = []\n",
    "        cTime = time.time()\n",
    "        time_diff = cTime - self.pTime\n",
    "        if time_diff > 0:\n",
    "            fps = 1 / time_diff\n",
    "            self.fps_label.setText(f\"FPS: {int(fps)}\")\n",
    "        self.pTime = cTime\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    mainWin = Ui_Dialog()\n",
    "    mainWin.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
